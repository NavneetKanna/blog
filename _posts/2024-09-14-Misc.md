---
layout: post
title: "Part 4: Miscellaneous"
date: 2024-09-14
---

## Softmax

- It converts logits to probability distributions
- It is differentiable 

## Cross Entropy Loss

- Difference between two probability distributions: the predicted distribution and the true distribution
- The logarithm function penalizes incorrect predictions more heavily than correct ones 
- Is differentiable

## Automatic vs Symbolic vs Numerical Differentiation 

- Numerical diff is using the [this](https://en.wikipedia.org/wiki/Derivative) formula to calculate the derivatives.
- Symbolic diff is using [these](https://www.pas.rochester.edu/~arijit/c02.pdf) formulas to calculate the derivatives.
- AD works by splitting the problem into elementary ops (computational graph) and using chain rule to calculate the derivates. There are two kinds:
    -  Forward mode AD: It starts at the input of the graph and moves towards the end.
    - Reverse mode AD: It starts at the end of the graph and moves towards the beginning. (source)[http://colah.github.io/posts/2015-08-Backprop/]