---
layout: post
title: "Part 4: Miscellaneous"
date: 2024-09-14
---

## Softmax

- It converts logits to probability distributions
- It is differentiable 

## Cross Entropy Loss

- Difference between two probability distributions: the predicted distribution and the true distribution
- The logarithm function penalizes incorrect predictions more heavily than correct ones 
- Is differentiable